{"cells":[{"cell_type":"markdown","metadata":{"id":"TGVmOeQRsBea"},"source":["### Setup environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MphLRESHsBef"},"outputs":[],"source":["import sys\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"Xzl_OPE-QuMi"},"source":["#### User defined function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"elapsed":13,"status":"error","timestamp":1690745689890,"user":{"displayName":"Tan Kean Yew","userId":"10234647363542763217"},"user_tz":-480},"id":"nVu69OboQuMi","outputId":"0ac1fb4d-e4da-41d7-8cff-3cf9fc55d521"},"outputs":[],"source":["from util_func import *"]},{"cell_type":"markdown","metadata":{"id":"WplckkVhsBei"},"source":["### Preprocessing of data\n","data obtain from http://www.nlpr.ia.ac.cn/pal/trafficdata/recognition.html"]},{"cell_type":"markdown","metadata":{"id":"tT2NSR3NsBei"},"source":["#### Step 1: Data loading\n","\n","##### The annotation meaning\n","\n","Example: 000_0001.png;134;128;19;7;120;117;0;\n","\n","- 000_0001.png: This is the name or identifier of the image associated with the traffic sign.\n","\n","The remaining values represent information about the traffic sign in the following order:\n","- 134: width of image\n","- 128: height of image\n","- 19: X-coordinate of the top-left corner of the traffic sign bounding box.\n","- 7: Y-coordinate of the top-left corner of the traffic sign bounding box.\n","- 120: X-coordinate of the bottom right corner\n","- 117: Y-coordinate of the bottom right corner\n","- 0: Class label or identifier of the traffic sign."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mP13g_bsBej"},"outputs":[],"source":["def load_img_annotation(img_path, annotation_path):\n","    # step 1: read the annotation\n","    # open the txt file in Read-mode (\"r\")\n","    with open(annotation_path, 'r') as file:\n","        annotation_list = file.read().splitlines() # split the txt file when endline, combine into a list\n","\n","    img_list = []\n","    img_namelist = []\n","    sorted_annotation_list = []\n","\n","    # step 2: load the image\n","    for image in os.listdir(img_path):\n","        img = cv.imread(os.path.join(img_path, image))\n","        if image is not None:\n","            img_list.append(img)\n","\n","        img_namelist.append(image)\n","\n","    # step 3: sort the annotation according to the image name\n","    for name in img_namelist:\n","        for annotation in annotation_list:\n","            a_name = annotation.split(\";\")[0] # split the annotation with semicolon, choose the first index\n","            if a_name == name: # if the first info from annotation match with the image name, then sort the annotation to front\n","                sorted_annotation_list.append(annotation)\n","\n","    return img_list, sorted_annotation_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"executionInfo":{"elapsed":17,"status":"error","timestamp":1690745725173,"user":{"displayName":"Tan Kean Yew","userId":"10234647363542763217"},"user_tz":-480},"id":"HZJw0lFMsBej","outputId":"202d6c7e-8f82-4390-fd1b-afc7c637e10e"},"outputs":[],"source":["# load dataset (train set and test set)\n","train_img_list, train_annotation_list = load_img_annotation(\"data/tsrd-train\", \"data\\TSRD-Train Annotation\\TsignRecgTrain4170Annotation.txt\")\n","test_img_list, test_annotation_list = load_img_annotation(\"data/TSRD-Test\", \"data\\TSRD-Test Annotation\\TsignRecgTest1994Annotation.txt\")"]},{"cell_type":"markdown","metadata":{"id":"8uNONEYAsBek"},"source":["#### Step 2: Crop image + Resizing\n","\n","Crop image based on the coordinate given by the annotation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zceYH3c7QuMl"},"outputs":[],"source":["# crop the image\n","def crop(img, top_left, bottom_right):\n","    return img[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"keHbXJfiQuMl"},"outputs":[],"source":["# crop the image based on the coordinate given\n","def img_crop(img_list, annotation_list):\n","    crop_img = []\n","    for i in range(len(img_list) - 1):\n","        tl_h, tl_w, br_h, br_w = annotation_list[i].split(\";\")[3:7]# top left height, top left width, bottom right height, bottom right width\n","\n","        cropped = crop(img_list[i], (int(tl_h), int(tl_w)), (int(br_h), int(br_w)))\n","        crop_img.append(cropped)\n","\n","    return crop_img"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"elapsed":371,"status":"error","timestamp":1690745907577,"user":{"displayName":"Tan Kean Yew","userId":"10234647363542763217"},"user_tz":-480},"id":"_ainrnlIQuMm","outputId":"c9e5d9d7-d863-49c4-93fb-48b43eaac4b9"},"outputs":[],"source":["cropped_img_train = img_crop(train_img_list, train_annotation_list)\n","cropped_img_test = img_crop(test_img_list, test_annotation_list)"]},{"cell_type":"markdown","metadata":{"id":"aUgdPFrDQuMm"},"source":["##### Resizing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3jyUctyQuMm"},"outputs":[],"source":["rs_train = []\n","rs_test = []\n","\n","# resize to (100, 100)\n","# for train set\n","for img in cropped_img_train:\n","    img_resized = cv.resize(img, (224, 224), interpolation=cv.INTER_LINEAR)\n","    rs_train.append(img_resized)\n","\n","# for test set\n","for img in cropped_img_test:\n","    img_resized = cv.resize(img, (224, 224), interpolation=cv.INTER_LINEAR)\n","    rs_test.append(img_resized)\n","\n","\n","# check\n","for i in range(20):\n","    show_img(\"resized\", rs_test[i])"]},{"cell_type":"markdown","metadata":{"id":"HrDtS_rUQuMm"},"source":["#####  Image Normalization\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KNjaK5QUqRc"},"outputs":[],"source":["def normalize_img(image_list):\n","    # Convert the list of images to a numpy array\n","    image_np = np.array(image_list)\n","\n","    # Perform normalization\n","    normalized_images = image_np.astype('float32') / 255.0\n","\n","    return normalized_images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrmWcNDLUyPV"},"outputs":[],"source":["rs_train_normalized = normalize_img(rs_train)\n","rs_test_normalized = normalize_img(rs_test)"]},{"cell_type":"markdown","metadata":{},"source":["Data augmentation"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'load_dataset' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpath/to/your/dataset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 27\u001b[0m images, labels \u001b[39m=\u001b[39m load_dataset(dataset_path)\n\u001b[0;32m     28\u001b[0m resized_images \u001b[39m=\u001b[39m resize_images(images)\n\u001b[0;32m     29\u001b[0m normalized_images \u001b[39m=\u001b[39m normalize_images(resized_images)\n","\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def augment_data(images, labels, augment_size=10000):\n","    datagen = ImageDataGenerator(\n","        rotation_range=20,  # Random rotation between -20 and 20 degrees\n","        width_shift_range=0.1,  # Random horizontal shift by 10% of the image width\n","        height_shift_range=0.1,  # Random vertical shift by 10% of the image height\n","        shear_range=0.2,  # Random shear transformation\n","        zoom_range=0.2,  # Random zoom between 80% and 120%\n","        horizontal_flip=True,  # Randomly flip images horizontally\n","        vertical_flip=False,  # No vertical\n","        fill_mode='nearest'  # Fill mode for filling in newly created pixels\n","    )\n","    augmented_images = []\n","    augmented_labels = []\n","    for image, label in zip(images, labels):\n","        image = image.reshape((1,) + image.shape)  # Expand dimensions for flow method\n","        i = 0\n","        for batch in datagen.flow(image, batch_size=1):\n","            if i >= augment_size:\n","                break\n","            augmented_images.append(batch[0])\n","            augmented_labels.append(label)\n","            i += 1\n","    return np.array(augmented_images), np.array(augmented_labels)\n","\n","# Example usage:\n","dataset_path = 'path/to/your/dataset'\n","images, labels = load_dataset(dataset_path)\n","resized_images = resize_images(images)\n","normalized_images = normalize_images(resized_images)\n","encoded_labels = preprocess_labels(labels)\n","augmented_images, augmented_labels = augment_data(normalized_images, encoded_labels)\n","\n","# Now you have the augmented data ready for training your CNN model.\n","# You can use augmented_images, augmented_labels for training."]},{"cell_type":"markdown","metadata":{"id":"dl1KQdPjc46e"},"source":["Data Labeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufLJhdZNc9Ix"},"outputs":[],"source":["def read_annotation_file(file_path):\n","    img_filenames = []\n","    class_labels = []\n","\n","    #open the file in read mode\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","\n","    for line in lines:\n","        info = line.strip().split(';')\n","        #Extract the filename in the 1st position in each line\n","        img_filename = info[0]\n","        #Extract the class label in the 8th position in each line\n","        class_label = int(info[7])\n","\n","        img_filenames.append(img_filename)\n","        class_labels.append(class_label)\n","\n","    return img_filenames, class_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4o6acoGoevJ1"},"outputs":[],"source":["img_filenames, class_labels = read_annotation_file(\"data\\TSRD-Train Annotation\\TsignRecgTrain4170Annotation.txt\")"]},{"cell_type":"markdown","metadata":{"id":"gRDSB3Bfhktg"},"source":["Data Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRuw5r__hoKc"},"outputs":[],"source":["def one_hot_encode(labels, num_classes):\n","    num_samples = len(labels)\n","    encoded_labels = np.zeros((num_samples, num_classes), dtype=np.float32)\n","\n","    for i, label in enumerate(labels):\n","        encoded_labels[i, label] = 1.0\n","\n","    return encoded_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HAsWivSi1cP"},"outputs":[],"source":["num_classes = 57\n","encoded_class_labels = one_hot_encode(class_labels, num_classes)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
