{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGVmOeQRsBea"
   },
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MphLRESHsBef"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzl_OPE-QuMi"
   },
   "source": [
    "#### User defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1690745689890,
     "user": {
      "displayName": "Tan Kean Yew",
      "userId": "10234647363542763217"
     },
     "user_tz": -480
    },
    "id": "nVu69OboQuMi",
    "outputId": "0ac1fb4d-e4da-41d7-8cff-3cf9fc55d521"
   },
   "outputs": [],
   "source": [
    "from util_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WplckkVhsBei"
   },
   "source": [
    "### Preprocessing of data\n",
    "data obtain from http://www.nlpr.ia.ac.cn/pal/trafficdata/recognition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT2NSR3NsBei"
   },
   "source": [
    "#### Step 1: Data loading\n",
    "\n",
    "##### The annotation meaning\n",
    "\n",
    "Example: 000_0001.png;134;128;19;7;120;117;0;\n",
    "\n",
    "- 000_0001.png: This is the name or identifier of the image associated with the traffic sign.\n",
    "\n",
    "The remaining values represent information about the traffic sign in the following order:\n",
    "- 134: width of image\n",
    "- 128: height of image\n",
    "- 19: X-coordinate of the top-left corner of the traffic sign bounding box.\n",
    "- 7: Y-coordinate of the top-left corner of the traffic sign bounding box.\n",
    "- 120: X-coordinate of the bottom right corner\n",
    "- 117: Y-coordinate of the bottom right corner\n",
    "- 0: Class label or identifier of the traffic sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8mP13g_bsBej"
   },
   "outputs": [],
   "source": [
    "def load_img_annotation(img_path, annotation_path):\n",
    "    # step 1: read the annotation\n",
    "    # open the txt file in Read-mode (\"r\")\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        annotation_list = file.read().splitlines() # split the txt file when endline, combine into a list\n",
    "\n",
    "    img_list = []\n",
    "    img_namelist = []\n",
    "    sorted_annotation_list = []\n",
    "\n",
    "    # step 2: load the image\n",
    "    for image in os.listdir(img_path):\n",
    "        img = cv.imread(os.path.join(img_path, image))\n",
    "        if image is not None:\n",
    "            img_list.append(img)\n",
    "\n",
    "        img_namelist.append(image)\n",
    "\n",
    "    # step 3: sort the annotation according to the image name\n",
    "    for name in img_namelist:\n",
    "        for annotation in annotation_list:\n",
    "            a_name = annotation.split(\";\")[0] # split the annotation with semicolon, choose the first index\n",
    "            if a_name == name: # if the first info from annotation match with the image name, then sort the annotation to front\n",
    "                sorted_annotation_list.append(annotation)\n",
    "\n",
    "    return img_list, sorted_annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "error",
     "timestamp": 1690745725173,
     "user": {
      "displayName": "Tan Kean Yew",
      "userId": "10234647363542763217"
     },
     "user_tz": -480
    },
    "id": "HZJw0lFMsBej",
    "outputId": "202d6c7e-8f82-4390-fd1b-afc7c637e10e"
   },
   "outputs": [],
   "source": [
    "# load dataset (train set and test set)\n",
    "train_img_list, train_annotation_list = load_img_annotation(\"data/tsrd-train\", \"data\\TSRD-Train Annotation\\TsignRecgTrain4170Annotation.txt\")\n",
    "test_img_list, test_annotation_list = load_img_annotation(\"data/TSRD-Test\", \"data\\TSRD-Test Annotation\\TsignRecgTest1994Annotation.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uNONEYAsBek"
   },
   "source": [
    "#### Step 2: Crop image + Resizing\n",
    "\n",
    "Crop image based on the coordinate given by the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zceYH3c7QuMl"
   },
   "outputs": [],
   "source": [
    "# crop the image\n",
    "def crop(img, top_left, bottom_right):\n",
    "    return img[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "keHbXJfiQuMl"
   },
   "outputs": [],
   "source": [
    "# crop the image based on the coordinate given\n",
    "def img_crop(img_list, annotation_list):\n",
    "    crop_img = []\n",
    "    for i in range(len(img_list) - 1):\n",
    "        tl_h, tl_w, br_h, br_w = annotation_list[i].split(\";\")[3:7]# top left height, top left width, bottom right height, bottom right width\n",
    "\n",
    "        cropped = crop(img_list[i], (int(tl_h), int(tl_w)), (int(br_h), int(br_w)))\n",
    "        crop_img.append(cropped)\n",
    "\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "error",
     "timestamp": 1690745907577,
     "user": {
      "displayName": "Tan Kean Yew",
      "userId": "10234647363542763217"
     },
     "user_tz": -480
    },
    "id": "_ainrnlIQuMm",
    "outputId": "c9e5d9d7-d863-49c4-93fb-48b43eaac4b9"
   },
   "outputs": [],
   "source": [
    "cropped_img_train = img_crop(train_img_list, train_annotation_list)\n",
    "cropped_img_test = img_crop(test_img_list, test_annotation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUgdPFrDQuMm"
   },
   "source": [
    "##### Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "U3jyUctyQuMm"
   },
   "outputs": [],
   "source": [
    "rs_train = []\n",
    "rs_test = []\n",
    "\n",
    "# resize to (100, 100)\n",
    "# for train set\n",
    "for img in cropped_img_train:\n",
    "    img_resized = cv.resize(img, (224, 224), interpolation=cv.INTER_LINEAR)\n",
    "    rs_train.append(img_resized)\n",
    "\n",
    "# for test set\n",
    "for img in cropped_img_test:\n",
    "    img_resized = cv.resize(img, (224, 224), interpolation=cv.INTER_LINEAR)\n",
    "    rs_test.append(img_resized)\n",
    "\n",
    "\n",
    "# check\n",
    "for i in range(20):\n",
    "    show_img(\"resized\", rs_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrDtS_rUQuMm"
   },
   "source": [
    "#####  Image Normalization\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9KNjaK5QUqRc"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image_list):\n",
    "    # Convert the list of images to a numpy array\n",
    "    image_np = np.array(image_list)\n",
    "\n",
    "    # Perform normalization\n",
    "    normalized_images = image_np.astype('float32') / 255.0\n",
    "\n",
    "    return normalized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XrmWcNDLUyPV"
   },
   "outputs": [],
   "source": [
    "rs_train_normalized = normalize_img(rs_train)\n",
    "rs_test_normalized = normalize_img(rs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl1KQdPjc46e"
   },
   "source": [
    "Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ufLJhdZNc9Ix"
   },
   "outputs": [],
   "source": [
    "def read_annotation_file(file_path):\n",
    "    img_filenames = []\n",
    "    class_labels = []\n",
    "\n",
    "    #open the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        info = line.strip().split(';')\n",
    "        #Extract the filename in the 1st position in each line\n",
    "        img_filename = info[0]\n",
    "        #Extract the class label in the 8th position in each line\n",
    "        class_label = int(info[7])\n",
    "\n",
    "        img_filenames.append(img_filename)\n",
    "        class_labels.append(class_label)\n",
    "\n",
    "    return img_filenames, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4o6acoGoevJ1"
   },
   "outputs": [],
   "source": [
    "img_filenames, class_labels = read_annotation_file(\"data\\TSRD-Train Annotation\\TsignRecgTrain4170Annotation.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRDSB3Bfhktg"
   },
   "source": [
    "Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mRuw5r__hoKc"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes + 1), dtype=np.float32)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded_labels[i, label] = 1.0\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-HAsWivSi1cP"
   },
   "outputs": [],
   "source": [
    "num_classes = 57\n",
    "encoded_class_labels = one_hot_encode(class_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(images, labels, augment_size=10000):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        image = np.expand_dims(image, axis=0)  # Expand dimensions for flow method\n",
    "\n",
    "        # Generate augmented images and labels\n",
    "        i = 0\n",
    "        for batch in datagen.flow(image, batch_size=1):\n",
    "            if i >= augment_size:\n",
    "                break\n",
    "            augmented_images.append(batch[0])\n",
    "            augmented_labels.append(label)\n",
    "            i += 1\n",
    "\n",
    "    augmented_images = np.array(augmented_images)\n",
    "    augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "    return augmented_images, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Data Augmentation to the training data\n",
    "augmented_images_train, augmented_labels_train = augment_data(rs_train_normalized, encoded_class_labels)\n",
    "\n",
    "# Combine original and augmented data for training\n",
    "rs_train_combined = np.concatenate((rs_train_normalized, augmented_images_train), axis=0)\n",
    "encoded_class_labels_combined = np.concatenate((encoded_class_labels, augmented_labels_train), axis=0)\n",
    "\n",
    "# Shuffle the combined data to avoid any bias\n",
    "combined_indices = np.arange(rs_train_combined.shape[0])\n",
    "np.random.shuffle(combined_indices)\n",
    "\n",
    "rs_train_combined = rs_train_combined[combined_indices]\n",
    "encoded_class_labels_combined = encoded_class_labels_combined[combined_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
